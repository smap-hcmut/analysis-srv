# PROPOSAL: IMPLEMENT ANALYTICS ORCHESTRATOR & ENTRY POINTS

**Module:** Core Orchestration
**Status:** Ready for Implementation
**Target Files:**

1.  `src/services/analytics/orchestrator.py` (Logic trung t√¢m)
2.  `src/entrypoints/consumer.py` (Lu·ªìng Production)
3.  `src/entrypoints/api.py` (Lu·ªìng Dev/Test)
4.  `src/adapters/storage/minio_client.py` (H·∫° t·∫ßng)

## 1\. Ki·∫øn tr√∫c T·ªïng th·ªÉ (High-Level Architecture)

Ch√∫ng ta s·ª≠ d·ª•ng m√¥ h√¨nh **Hexagonal Architecture** (Ports & Adapters). `AnalyticsOrchestrator` n·∫±m ·ªü gi·ªØa, kh√¥ng quan t√¢m d·ªØ li·ªáu ƒë·∫øn t·ª´ Queue hay API, mi·ªÖn l√† ƒë√∫ng format JSON.

[Image of analytics pipeline data flow]

```mermaid
graph TD
    subgraph "Entry Points (Adapters)"
        Queue[RabbitMQ Consumer] -->|1a. Receive Msg| MinIO_Client
        MinIO_Client -->|1b. Download JSON| Orchestrator

        API[Test API Endpoint] -->|2. Receive JSON Body| Orchestrator
    end

    subgraph "Core Business Logic"
        Orchestrator[Analytics Orchestrator]

        Orchestrator --> Module1[1. Preprocessor]
        Orchestrator --> Module2[2. Intent]
        Orchestrator --> Module3[3. Keyword]
        Orchestrator --> Module4[4. Sentiment]
        Orchestrator --> Module5[5. Impact]
    end

    subgraph "Persistence"
        Orchestrator --> Repo[Analytics Repository]
        Repo --> DB[(PostgreSQL)]
    end
```

---

## 2\. Thi·∫øt k·∫ø Chi ti·∫øt (Implementation Details)

### 2.1. MinIO Adapter (C·∫ßu n·ªëi d·ªØ li·ªáu)

C·∫ßn m·ªôt client ƒë·ªÉ t·∫£i file JSON t·ª´ "Kho" v·ªÅ "Nh√† m√°y" d·ª±a tr√™n t√≠n hi·ªáu t·ª´ Queue.

**File:** `src/adapters/storage/minio_client.py`

```python
import json
from minio import Minio
from src.core.config import settings

class MinioAdapter:
    def __init__(self):
        self.client = Minio(
            settings.MINIO_ENDPOINT,
            access_key=settings.MINIO_ACCESS_KEY,
            secret_key=settings.MINIO_SECRET_KEY,
            secure=False
        )

    def download_json(self, bucket: str, object_path: str) -> dict:
        """
        Stream file t·ª´ MinIO v·ªÅ RAM v√† parse th√†nh Dict.
        Kh√¥ng l∆∞u temp file ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô.
        """
        try:
            response = self.client.get_object(bucket, object_path)
            data = json.load(response)
            response.close()
            response.release_conn()
            return data
        except Exception as e:
            raise Exception(f"Failed to fetch from MinIO: {str(e)}")
```

---

### 2.2. The Orchestrator (Tr√°i tim h·ªá th·ªëng)

ƒê√¢y l√† class duy nh·∫•t ch·ª©a logic ph·ªëi h·ª£p 5 module. N√≥ nh·∫≠n v√†o `post_data` (Dict) v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ ƒë√£ x·ª≠ l√Ω.

**File:** `src/services/analytics/orchestrator.py`

```python
import logging
from sqlalchemy.orm import Session
# Import 5 modules & Repo (nh∆∞ ƒë√£ c√≥)

logger = logging.getLogger(__name__)

class AnalyticsOrchestrator:
    def __init__(self, db: Session, phobert_model):
        self.db = db
        # Init 5 Modules (Singleton Instances)
        self.preprocessor = TextPreprocessor()
        self.intent_classifier = IntentClassifier()
        self.keyword_extractor = KeywordExtractor()
        self.sentiment_analyzer = SentimentAnalyzer(phobert_model)
        self.impact_calculator = ImpactCalculator()
        self.repo = AnalyticsRepository(db)

    def process_post(self, post_data: dict) -> dict:
        """
        H√†m x·ª≠ l√Ω trung t√¢m.
        Input: Atomic JSON c·ªßa 1 b√†i vi·∫øt.
        Output: K·∫øt qu·∫£ ph√¢n t√≠ch ƒë√£ l∆∞u DB.
        """
        post_id = post_data.get('meta', {}).get('id')
        logger.info(f"üöÄ Starting pipeline for post: {post_id}")

        # --- STEP 1: PREPROCESS ---
        prep_result = self.preprocessor.process(post_data)
        clean_text = prep_result['clean_text']

        # --- STEP 2: INTENT (GATEKEEPER) ---
        intent_result = self.intent_classifier.predict(clean_text)

        # Logic l·ªçc r√°c k·∫øt h·ª£p (Signal t·ª´ Module 1 + Module 2)
        is_spam_signal = prep_result['stats'].get('has_spam_keyword', False)
        if is_spam_signal or intent_result['should_skip']:
            logger.info(f"‚õî Skipped SPAM/SEEDING post: {post_id}")
            return self._save_skipped_result(post_data, intent_result)

        # --- STEP 3: KEYWORD EXTRACTION ---
        keywords = self.keyword_extractor.extract(clean_text)

        # --- STEP 4: SENTIMENT ANALYSIS ---
        sentiment_result = self.sentiment_analyzer.analyze(clean_text, keywords)

        # --- STEP 5: IMPACT CALCULATION ---
        impact_result = self.impact_calculator.calculate(
            interaction=post_data.get('interaction', {}),
            author=post_data.get('author', {}),
            sentiment_result=sentiment_result['overall'],
            platform=post_data.get('meta', {}).get('platform', 'UNKNOWN')
        )

        # --- STEP 6: ASSEMBLY & SAVE ---
        final_result = {
            "id": post_id,
            "project_id": post_data['meta']['project_id'],
            # ... map c√°c tr∆∞·ªùng kh√°c ...
            "overall_sentiment": sentiment_result['overall']['label'],
            "impact_score": impact_result['score'],
            # ...
        }

        self.repo.save(final_result)
        logger.info(f"‚úÖ Successfully processed post: {post_id}")
        return final_result

    def _save_skipped_result(self, post_data, intent_result):
        # L∆∞u v√†o DB nh∆∞ng ƒë√°nh d·∫•u l√† skipped/spam
        pass
```

---

### 2.3. Entry Point 1: RabbitMQ Consumer (Production Flow)

Lu·ªìng n√†y l·∫Øng nghe Queue, l·∫•y path, g·ªçi MinIO, r·ªìi n√©m c·ª•c JSON v√†o Orchestrator.

**File:** `src/entrypoints/consumer.py`

```python
import json
from src.adapters.storage.minio_client import MinioAdapter
from src.services.analytics.orchestrator import AnalyticsOrchestrator

def callback(ch, method, properties, body):
    try:
        # 1. Parse Message t·ª´ Queue
        msg = json.loads(body)
        bucket = msg['data_ref']['bucket']
        path = msg['data_ref']['path']

        print(f"üì• Received Job for: {path}")

        # 2. L·∫•y JSON t·ª´ MinIO
        minio_adapter = MinioAdapter()
        post_data = minio_adapter.download_json(bucket, path)

        # 3. G·ªçi Orchestrator x·ª≠ l√Ω
        # (Gi·∫£ s·ª≠ db_session v√† model ƒë√£ ƒë∆∞·ª£c init global ho·∫∑c inject v√†o)
        orchestrator = AnalyticsOrchestrator(db_session, phobert_model)
        orchestrator.process_post(post_data)

        # 4. Acknowledge (X√°c nh·∫≠n th√†nh c√¥ng)
        ch.basic_ack(delivery_tag=method.delivery_tag)

    except Exception as e:
        print(f"‚ùå Error processing message: {e}")
        # Nack ƒë·ªÉ queue g·ª≠i l·∫°i sau (ho·∫∑c ƒë·∫©y v√†o DLQ)
        ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)
```

---

### 2.4. Entry Point 2: REST API (Dev/Test Flow)

Lu·ªìng n√†y nh·∫≠n JSON tr·ª±c ti·∫øp t·ª´ Body, b·ªè qua MinIO. C·ª±c ti·ªán ƒë·ªÉ debug logic AI m√† kh√¥ng c·∫ßn upload file.

**File:** `src/entrypoints/api.py`

```python
from fastapi import APIRouter, Depends
from src.services.analytics.orchestrator import AnalyticsOrchestrator

router = APIRouter()

@router.post("/dev/process-post-direct")
async def dev_process_post_direct(
    post_data: dict,  # Nh·∫≠n nguy√™n c·ª•c JSON Atomic
    orchestrator: AnalyticsOrchestrator = Depends(get_orchestrator)
):
    """
    API d√†nh cho Dev/Test.
    Input: JSON b√†i vi·∫øt (kh√¥ng c·∫ßn upload MinIO).
    Output: K·∫øt qu·∫£ ph√¢n t√≠ch (ƒë√£ l∆∞u DB).
    """
    try:
        result = orchestrator.process_post(post_data)
        return {
            "status": "SUCCESS",
            "data": result
        }
    except Exception as e:
        return {
            "status": "FAILED",
            "error": str(e)
        }
```

---

## 3\. K·∫ø ho·∫°ch Ki·ªÉm th·ª≠ (Testing Plan)

### Case 1: Test API Flow (D·ªÖ nh·∫•t)

1.  M·ªü Postman / Swagger UI.
2.  G·ªçi `POST /dev/process-post-direct`.
3.  Paste n·ªôi dung file `examples/sample_post.json` v√†o body.
4.  B·∫•m Send.
5.  **K·ª≥ v·ªçng:** Nh·∫≠n v·ªÅ JSON k·∫øt qu·∫£ ph√¢n t√≠ch ƒë·∫ßy ƒë·ªß. Check DB th·∫•y c√≥ record m·ªõi.

### Case 2: Test Production Flow (Integration)

1.  Upload th·ªß c√¥ng 1 file `test.json` l√™n MinIO bucket `raw-data`.
2.  V√†o RabbitMQ Management UI, publish th·ªß c√¥ng 1 message v√†o queue `analytics.process.queue`:
    ```json
    {
      "data_ref": {
        "bucket": "raw-data",
        "path": "test.json"
      }
    }
    ```
3.  **K·ª≥ v·ªçng:** Consumer log ra d√≤ng "üì• Received Job...", sau ƒë√≥ "‚úÖ Successfully processed". Check DB th·∫•y record.

---

### K·∫øt lu·∫≠n

V·ªõi Proposal n√†y, b·∫°n c√≥ m·ªôt h·ªá th·ªëng **Linh ho·∫°t tuy·ªát ƒë·ªëi**:

- **Production:** Ch·∫°y Async qua Queue, Scale tho·∫£i m√°i.
- **Development:** Ch·∫°y Sync qua API, Debug l·ªói logic ngay l·∫≠p t·ª©c m√† kh√¥ng c·∫ßn setup Queue/MinIO ph·ª©c t·∫°p m·ªói l·∫ßn test m·ªôt case nh·ªè.

H√£y ƒë∆∞a proposal n√†y cho Agent ƒë·ªÉ ho√†n thi·ªán m·∫£nh gh√©p cu·ªëi c√πng\!
